NNPI Resnet50 Post-Training Quantization
----------------------------------------

NNPI Resnet50 post-training quantization requires a dynamic range for each weight and 
activation tensor. At present, NNPI Resnet50 quantization uses SYMLOWP 8-bit representation for both.

Quantization
------------
We assume a Gaussian distribution and use this knowledge to restrict the quantization range to a width of 3.92
standard deviations from the mean. A lot of research has shown that the Gaussian assumption is a common approximation (e.g., Soudry, D., Hubara, I., and Meir, R.
Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In Advances in Neural Information Processing Systems, pp. 963â€“971, 2014), 
based on the fact that the neural input x(d) is a sum of many inputs, 
so we expect it to be approximately Gaussian from the central limit theorem.